{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f7791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import undetected_chromedriver as webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1592ab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# brighton_postcodes = ['BN1', 'BN13', 'BN17', 'BN21', 'BN25', 'BN41', 'BN45', 'BN8', 'BN10', 'BN14', 'BN18', 'BN22',\n",
    "#              'BN26', 'BN42', 'BN5', 'BN9', 'BN11', 'BN15', 'BN2', 'BN23', 'BN27', 'BN43', 'BN6', 'BN12',\n",
    "#              'BN16', 'BN20', 'BN24', 'BN3', 'BN44', 'BN7']\n",
    "\n",
    "london_postcodes = pd.read_csv('./Data/London postcode districts.csv', header=0)\n",
    "all_postcodes = london_postcodes['Postcode district'].tolist()\n",
    "gotten_postcodes = pd.read_csv('./Data/gotten_postcodes_london.csv')['postcode'].tolist()\n",
    "gotten_postcodes\n",
    "\n",
    "postcodes = []\n",
    "for post in all_postcodes:\n",
    "    if post not in gotten_postcodes:\n",
    "        postcodes.append(post)\n",
    "print(len(postcodes))\n",
    "postcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e99e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = pymysql.connect(\n",
    "            host=\"localhost\",          # Your host name\n",
    "            user=\"root\",      # Your username\n",
    "            password=\"admin\",  # Your password\n",
    "            database=\"property_info_db\"   # Your database name\n",
    "        )\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb9f35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_property_links_to_db(data, db_name):\n",
    "    \"\"\"\n",
    "    Saves postcode and propert link to database\n",
    "    \"\"\"\n",
    "    cur.execute(f\"\"\"INSERT INTO {db_name} (\n",
    "        postcode, url) VALUES (%s, %s)\"\"\", (\n",
    "        data['postcode'],\n",
    "        data['url']\n",
    "    ))\n",
    "    \n",
    "    conn.commit()\n",
    "    print(\"Saved data to db successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaf7870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_property_links(postcode):\n",
    "    \"\"\"\n",
    "    retrieves the property links from the zoopla website\n",
    "    :param postcodes: a list containing string representation of a postcode district\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "#     urls_list = []\n",
    "#     df = pd.DataFrame()\n",
    "    data = {\n",
    "        'url': \"\",\n",
    "        'postcode': \"\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        s = Service(\"C:\\\\Users\\\\user\\\\Downloads\\\\Drivers\\\\chromedriver_win32\\\\chromedriver.exe\")\n",
    "        url = 'https://www.zoopla.co.uk/'\n",
    "        \n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument('--headless')\n",
    "        chrome_options.add_argument('--disable-gpu') \n",
    "\n",
    "        driver = webdriver.Chrome(service=s, options=chrome_options)\n",
    "        driver.get(url)\n",
    "\n",
    "        # Maximize the window\n",
    "        driver.maximize_window()\n",
    "\n",
    "        time.sleep(5)\n",
    "        consent_id = \"gdpr-consent-notice\"\n",
    "\n",
    "        iframe = driver.find_element(By.ID, consent_id)\n",
    "\n",
    "        # Switch to the iframe\n",
    "        driver.switch_to.frame(iframe)\n",
    "\n",
    "        # Click the accept button\n",
    "        time.sleep(2)\n",
    "        accept_cookies_btn = driver.find_element(By.XPATH, \"//button[@id='save']\")\n",
    "        accept_cookies_btn.click()\n",
    "\n",
    "        # switch back to main content\n",
    "        driver.switch_to.default_content()\n",
    "\n",
    "        # Find the search bar\n",
    "        time.sleep(2)\n",
    "        search = driver.find_element(By.XPATH, \"//input[contains(@id,'downshift')]\")\n",
    "        search.send_keys(postcode)\n",
    "\n",
    "        # Click the search button\n",
    "        time.sleep(2)\n",
    "        driver.find_element(By.XPATH, \"//button[@class='x8jo560 x8jo562 x8jo56a _16fktr8']\").click()\n",
    "\n",
    "        # Saving data\n",
    "        time.sleep(5)\n",
    "        # get total number of properties in postcode\n",
    "        total_properties = (\n",
    "            int(driver.find_element(By.XPATH, '//p[@data-testid=\"total-results\"]').text.strip('results').strip('')))\n",
    "        print(f'There are {total_properties} properties in {postcode}')\n",
    "\n",
    "        # # page index\n",
    "        count = 0\n",
    "        number_of_pages = (total_properties // 25) + 1\n",
    "        i = 1\n",
    "\n",
    "        while i <= number_of_pages:\n",
    "            listings = driver.find_elements(By.XPATH, '//a[@class=\"_1maljyt1\"]')\n",
    "#             print(f'The length is {len(listings)}')\n",
    "            print(f'Scrapping page {i} of {number_of_pages} from postcode {postcode}')\n",
    "            for property_link in listings:\n",
    "                count += 1\n",
    "                link = property_link.get_attribute('href')\n",
    "                data['postcode'] = postcode\n",
    "                data['url'] = link or \"\"\n",
    "                save_property_links_to_db(data, \"london_properties_links\")\n",
    "                data['postcode'] = \"\"\n",
    "                data['url'] = \"\"\n",
    "                print(f\"link {count}: {link}\")\n",
    "            i += 1\n",
    "            url = f'https://www.zoopla.co.uk/for-sale/property/{postcode}/?q={postcode}&search_source=home&pn={i}'\n",
    "            driver.get(url)\n",
    "            time.sleep(3)\n",
    "        print(f'Scraping completed for {postcode}')\n",
    "    except ElementClickInterceptedException:\n",
    "        print(f'code failing for postcode {postcode} due to element being intercepted')\n",
    "    except TimeoutException:\n",
    "        print(f'code failing for postcode {postcode} due to timeout')\n",
    "    except NoSuchElementException:\n",
    "        print(f'code failing for postcode {postcode} due to element not being found')\n",
    "    finally:\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041b215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for postcode in postcodes:\n",
    "    try:\n",
    "        get_property_links(postcode)\n",
    "    except Exception as ex:\n",
    "        print(f\"Error occurred : {ex}\")\n",
    "        continue\n",
    "print('Done with all postcodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3163668",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_postcodes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4287fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_postcodes_list = []\n",
    "postcodes = pd.read_csv('./Data/bad_postcodes.csv', index_col=[0])\n",
    "# print(postcodes.head())\n",
    "df_existing = pd.read_csv('./Data/property_urls.csv', index_col=[0])\n",
    "# print(df_existing.head())\n",
    "\n",
    "for postcode in postcodes['bad_postcodes'].values:\n",
    "    try:\n",
    "        df_postcode = get_property_links(postcode)\n",
    "        df_existing = pd.concat([df_existing, df_postcode], ignore_index=True)\n",
    "    except ElementClickInterceptedException:\n",
    "        continue\n",
    "    except TimeoutException:\n",
    "        continue\n",
    "    except NoSuchElementException:\n",
    "        continue\n",
    "    finally:\n",
    "        df_existing.to_csv('./Data/property_urls_updated.csv')\n",
    "        pd.DataFrame({'bad_postcodes':bad_postcodes_list}).to_csv('./Data/bad_postcodes_2.csv')\n",
    "print('Done with all postcodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e454a200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_property_info(property_link, postcode) -> dict:\n",
    "    \"\"\"\n",
    "    Retrieves the information of a property\n",
    "    :param property_link: a string representing the property page\n",
    "    :param postcode: a string representing the property's postcode\n",
    "    :return: data_dict: A dictionary containing the property data\n",
    "    \"\"\"\n",
    "    data_dict = {\n",
    "        \"Price\": \"\",\n",
    "        \"Address\": \"\",\n",
    "        \"House Type\": \"\",\n",
    "        \"Number of Bedrooms\": \"\",\n",
    "        \"Number of Bathrooms\": \"\",\n",
    "        \"Number of Receptions\": \"\",\n",
    "        \"Other Features\": \"\",\n",
    "        \"Tenure\": \"\",\n",
    "        \"Lease Time\": \"\",\n",
    "        \"Service Charge\": \"\",\n",
    "        \"Tax Band\": \"\",\n",
    "        \"Ground Rent\": \"\",\n",
    "        \"Commonhold Details\": \"\",\n",
    "        \"Points of Interest\": \"\",\n",
    "        \"Listing Features\": \"\",\n",
    "        \"Description\": \"\",\n",
    "        \"Property Link\": property_link,\n",
    "        \"Postcode\": postcode\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        s = Service(\"C:\\\\Users\\\\user\\\\Downloads\\\\Drivers\\\\chromedriver_win32\\\\chromedriver.exe\")\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument('--headless')\n",
    "        chrome_options.add_argument('--disable-gpu') \n",
    "\n",
    "        driver = webdriver.Chrome(service=s, options=chrome_options)\n",
    "        driver.get(property_link)\n",
    "\n",
    "        # Maximize the window\n",
    "        # time.sleep(2)\n",
    "        driver.maximize_window()\n",
    "\n",
    "        time.sleep(5)\n",
    "        consent_id = \"gdpr-consent-notice\"\n",
    "\n",
    "        iframe = driver.find_element(By.ID, consent_id)\n",
    "\n",
    "        # Switch to the iframe\n",
    "        driver.switch_to.frame(iframe)\n",
    "\n",
    "        # Click the accept button\n",
    "        time.sleep(2)\n",
    "        accept_cookies_btn = driver.find_element(By.XPATH, \"//button[@id='save']\")\n",
    "        accept_cookies_btn.click()\n",
    "\n",
    "        # switch back to main content\n",
    "        driver.switch_to.default_content()\n",
    "\n",
    "        # time.sleep(2)\n",
    "\n",
    "        # get price\n",
    "        data_dict[\"Price\"] = driver.find_element(By.XPATH, '//p[@data-testid=\"price\"]').text or np.nan\n",
    "\n",
    "        # address\n",
    "        data_dict[\"Address\"] = driver.find_element(By.XPATH, '//address[@data-testid=\"address-label\"]').text or np.nan\n",
    "\n",
    "        # house type\n",
    "        data_dict[\"House Type\"] = (driver.find_element(By.XPATH, '//div[@data-testid=\"title-label\"]').text.lower()\n",
    "                      .replace('for sale', \"\").strip() or np.nan)\n",
    "\n",
    "        # features\n",
    "        for feature in driver.find_elements(By.XPATH, '//div[@class=\"_1fuu7p80 _1fuu7p85 _1dgm2fc8 \"]'):\n",
    "            # Number of Bedrooms\n",
    "            if 'bed' in feature.text.lower():\n",
    "                data_dict[\"Number of Bedrooms\"] = feature.text or np.nan\n",
    "            # Number of Bathrooms\n",
    "            elif 'bath' in feature.text.lower(): \n",
    "                data_dict[\"Number of Bathrooms\"] = feature.text or np.nan\n",
    "            # Number of Receptions\n",
    "            elif 'reception' in feature.text.lower():\n",
    "                data_dict[\"Number of Receptions\"] = feature.text or np.nan\n",
    "            # Other Features\n",
    "            else:\n",
    "                data_dict[\"Other Features\"] = data_dict.get(\"Other Features\", \"\") + str(feature.text) + ','\n",
    "        data_dict[\"Other Features\"] = data_dict.get(\"Other Features\", \"\").rstrip(',')\n",
    "\n",
    "        # contract info\n",
    "        for info in driver.find_elements(By.XPATH, '//div[@class=\"_1k66bqh0\"]'):\n",
    "            # Tenure\n",
    "            if 'tenure' in info.text.lower():\n",
    "                data_dict[\"Tenure\"] = info.find_element(By.XPATH, \"following-sibling::*[1]\").text or np.nan\n",
    "            # Least Time\n",
    "            elif 'lease' in info.text.lower():\n",
    "                data_dict[\"Lease Time\"] = info.find_element(By.XPATH, 'following-sibling::*[1]').text or np.nan\n",
    "            # Service Charge\n",
    "            elif 'service' in info.text.lower():\n",
    "                data_dict[\"Service Charge\"] = info.find_element(By.XPATH, 'following-sibling::*[1]').text or np.nan\n",
    "            # Tax Band\n",
    "            elif 'tax' in info.text.lower():\n",
    "                data_dict[\"Tax Band\"] = info.find_element(By.XPATH, 'following-sibling::*[1]').text or np.nan\n",
    "            # Ground Rent\n",
    "            elif 'rent' in info.text.lower():\n",
    "                data_dict[\"Ground Rent\"] = info.find_element(By.XPATH, 'following-sibling::*[1]').text or np.nan\n",
    "            # Commonhold\n",
    "            elif 'commonhold' in info.text.lower():\n",
    "                data_dict[\"Commonhold Details\"] = info.find_element(By.XPATH, 'following-sibling::*[1]').text or np.nan\n",
    "\n",
    "        # points of interest\n",
    "        for interest in driver.find_elements(By.XPATH, '//li[@class=\"cx0jbd2\"]'):\n",
    "            data_dict[\"Points of Interest\"] = data_dict.get(\"Points of Interest\", \"\") + str(interest.text) + ','\n",
    "        data_dict[\"Points of Interest\"] = data_dict.get(\"Points of Interest\", \"\").rstrip(',')\n",
    "\n",
    "        # listing features\n",
    "        for listing_feature in driver.find_elements(By.XPATH, '//li[@class=\"swbww71\"]'):\n",
    "            data_dict[\"Listing Features\"] = data_dict.get(\"Listing Features\", \"\") + listing_feature.text + '\\n'\n",
    "\n",
    "        # description\n",
    "        data_dict[\"Description\"] = driver.find_element(By.XPATH, '//div[@class=\"ru2q7m3\"]').text or np.nan\n",
    "        \n",
    "    except Exception as ex:\n",
    "        print(f'An error occurred for link in get_data_func: {property_link}: {ex}')\n",
    "#         bad_link = {'Link':property_link, 'postcode': postcode}\n",
    "#         bad_links.append(bad_link)\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        time.sleep(2)\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73503eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_db(data):\n",
    "    \"\"\"\n",
    "    Saves extracted data to an sql database\n",
    "    :param data\n",
    "    \"\"\"\n",
    "    cur.execute(\"\"\"INSERT INTO london_properties (\n",
    "        price, address, house_type, number_of_bedrooms, number_of_bathrooms, number_of_receptions, other_features, tenure, \n",
    "        lease_time, service_charge, tax_band, ground_rent, commonhold_details, points_of_interest, listing_features, \n",
    "        description_text, property_link, postcode) \n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\", (\n",
    "    data['Price'],\n",
    "    data['Address'],\n",
    "    data['House Type'],\n",
    "    data['Number of Bedrooms'],\n",
    "    data['Number of Bathrooms'],\n",
    "    data['Number of Receptions'],\n",
    "    data['Other Features'],\n",
    "    data['Tenure'],\n",
    "    data['Lease Time'],\n",
    "    data['Service Charge'],\n",
    "    data['Tax Band'],\n",
    "    data['Ground Rent'],\n",
    "    data['Commonhold Details'],\n",
    "    data['Points of Interest'],\n",
    "    data['Listing Features'],\n",
    "    data['Description'],\n",
    "    data['Property Link'],\n",
    "    data['Postcode']))\n",
    "    \n",
    "    conn.commit()\n",
    "    print(\"Saved to db successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab220608",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_data = pd.read_csv('./Data/property_urls_london.csv', index_col = [0], skiprows=25135, header=None)\n",
    "property_data.columns = ['postcode', 'url']\n",
    "lk = \"https://www.zoopla.co.uk/new-homes/details/64887867/?search_identifier=a1b4f845cf6cbe628a251c0f240f4c7a9e328c159d98a3cb03d738ff2bf731d0\"\n",
    "property_data[property_data['url'] == lk]\n",
    "# property_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4d9baf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List of property links and corresponding postcodes\n",
    "property_data = pd.read_csv('./Data/property_urls_london.csv', index_col = [0], skiprows=25136, header=None)\n",
    "property_data.columns = ['postcode', 'url']\n",
    "# Initialize a list to store the scraped data\n",
    "# data_list = []\n",
    "\n",
    "# Bad links\n",
    "# bad_links = []\n",
    "\n",
    "# counter\n",
    "count = 25135\n",
    "\n",
    "# Loop through each property link and postcode\n",
    "for row in property_data.itertuples():\n",
    "    count += 1\n",
    "    try:\n",
    "        print(f\"Starting extraction for link {count}: {row.url}\")\n",
    "        data_dict = get_property_info(row.url, row.postcode)\n",
    "        save_to_db(data_dict)\n",
    "        print(f\"Done with link {count}: {row.url}\")\n",
    "#         break\n",
    "    except Exception as ex:\n",
    "        print(f'An error occurred for link {count} {row.url}: {ex}')\n",
    "        bad_link = {'Link':row.url, 'postcode': row.postcode}\n",
    "#         bad_links.append(bad_link)\n",
    "        continue\n",
    "\n",
    "print(\"Completed Data Extraction\")\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "# df = pd.DataFrame(data_list)\n",
    "\n",
    "# Display the DataFrame\n",
    "# print(df)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "# df.to_csv(\"./Data/scraped_property_data_for_Brighton.csv\", index=False)\n",
    "\n",
    "# Save Bad Links\n",
    "# pd.DataFrame(bad_links).to_csv(\"./Data/bad_links.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98108f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c686d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_links = pd.read_csv('./Data/extracted_data_brighon.csv', usecols=[0])\n",
    "all_the_links = pd.read_csv('./Data/property_urls_updated.csv', index_col = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f52028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = all_the_links['urls'].tolist()\n",
    "links_extracted = current_links['property_link'].tolist()\n",
    "\n",
    "links_to_get_list =  []\n",
    "\n",
    "for link_to_get in all_links:\n",
    "    if link_to_get not in links_extracted:\n",
    "        links_to_get_list.append(True)\n",
    "    else:\n",
    "        links_to_get_list.append(False)\n",
    "links_to_get_list\n",
    "len(links_to_get_list)\n",
    "links_to_get = all_the_links.loc[links_to_get_list]\n",
    "links_to_get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aa9d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter\n",
    "count = 0\n",
    "\n",
    "# Loop through each property link and postcode\n",
    "for row in links_to_get.itertuples():\n",
    "    count += 1\n",
    "    try:\n",
    "        data_dict = get_property_info(row.urls, row.postcode)\n",
    "        save_to_db(data_dict)   \n",
    "    except Exception as ex:\n",
    "        print(f'An error occurred for link {count} {row.urls}: {ex}')\n",
    "        continue\n",
    "    finally:\n",
    "        print(f\"Done with link {count}: {row.urls}\")\n",
    "\n",
    "print(\"Completed Data Extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6687ad7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
